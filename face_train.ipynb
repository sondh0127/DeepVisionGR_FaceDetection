{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-03-09T14:33:21.367021Z",
          "start_time": "2018-03-09T22:33:12.637484+08:00"
        },
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": "import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom keras.optimizers import Adam, SGD, Nadam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, LearningRateScheduler\nfrom keras.callbacks import Callback\nfrom keras import backend as K \nfrom keras.models import load_model\nfrom math import ceil \nimport numpy as np \nfrom termcolor import colored\n\nfrom mn_model import mn_model\nfrom face_generator import BatchGenerator\nfrom keras_ssd_loss import SSDLoss\nfrom ssd_box_encode_decode_utils import SSDBoxEncoder, decode_y, decode_y2\n\nimport scipy.misc as sm\nimport os\nos.environ[\u0027CUDA_VISIBLE_DEVICES\u0027] \u003d \u00270\u0027 # choose gpu"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-03-09T14:33:21.378011Z",
          "start_time": "2018-03-09T22:33:21.368987+08:00"
        },
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "# Input image\n",
        "img_height \u003d 512\n",
        "img_width \u003d 512\n",
        "img_channels \u003d 3\n",
        "\n",
        "n_classes \u003d 2 \n",
        "class_names \u003d [\"background\",\"face\"]\n",
        "\n",
        "scales \u003d [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # anchorboxes for coco dataset\n",
        "aspect_ratios \u003d [[0.5, 1.0, 2.0],\n",
        "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
        "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
        "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
        "                 [0.5, 1.0, 2.0],\n",
        "                 [0.5, 1.0, 2.0]] # The anchor box aspect ratios used in the original SSD300\n",
        "two_boxes_for_ar1 \u003d True\n",
        "limit_boxes \u003d True # Whether or not you want to limit the anchor boxes to lie entirely within the image boundaries\n",
        "variances \u003d [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are scaled as in the original implementation\n",
        "coords \u003d \u0027centroids\u0027 # Whether the box coordinates to be used as targets for the model should be in the \u0027centroids\u0027 or \u0027minmax\u0027 format, see documentation\n",
        "normalize_coords \u003d True\n",
        "\n",
        "det_model_path \u003d \"./models/\" \n",
        "train_data \u003d \u0027wider_train_small.npy\u0027\n",
        "test_data \u003d \u0027wider_val_small.npy\u0027\n",
        "data_path \u003d \u0027../../data/\u0027"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-03-09T14:33:35.285228Z",
          "start_time": "2018-03-09T22:33:21.379013+08:00"
        },
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Height, Width, Channels : 512 512 3\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Freezing classification layers\n",
            "\u001b[32mclassification layers freezed\u001b[0m\n",
            "loading classification weights\n",
            "\u001b[32mclassification weights ./base_models/mobilenet_1_0_224_tf.h5 loaded\u001b[0m\n"
          ]
        }
      ],
      "source": "# build the keras model\n# this model is not retrained, we are doing it from scratch \n\nK.clear_session()\n\nmodel, model_layer, img_input, predictor_sizes \u003d mn_model(image_size\u003d(img_height, img_width, img_channels), \n                                                                      n_classes \u003d n_classes,\n                                                                      min_scale \u003d None, \n                                                                      max_scale \u003d None, \n                                                                      scales \u003d scales, \n                                                                      aspect_ratios_global \u003d None, \n                                                                      aspect_ratios_per_layer \u003d aspect_ratios, \n                                                                      two_boxes_for_ar1\u003d two_boxes_for_ar1, \n                                                                      limit_boxes\u003dlimit_boxes, \n                                                                      variances\u003d variances, \n                                                                      coords\u003dcoords, \n                                                                      normalize_coords\u003dnormalize_coords)\n\n#model.summary()\n\nprint (\"Freezing classification layers\")\n#Freeze layers\nfor layer_key in model_layer:\n  if(\u0027detection\u0027  not in layer_key): #prefix detection to freeze layers which does not have detection\n    model_layer[layer_key].trainable \u003d False\nprint (colored(\"classification layers freezed\", \u0027green\u0027))\n\n# for layer in model.layers:\n#   print (colored(layer.name, \u0027blue\u0027))\n#   print (colored(layer.trainable, \u0027green\u0027))\n\nprint (\"loading classification weights\")\nclassification_model \u003d \u0027./base_models/mobilenet_1_0_224_tf.h5\u0027\nmodel.load_weights(mobilenet,  by_name\u003d True)\n\nprint (colored( (\u0027classification weights %s loaded\u0027 % classification_model), \u0027green\u0027))"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-03-09T14:36:54.861936Z",
          "start_time": "2018-03-09T22:33:35.286188+08:00"
        },
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING DATA\n",
            "Total number of training samples \u003d 128\n",
            "VALIDATION DATA\n",
            "Total number of validation samples \u003d 60\n"
          ]
        }
      ],
      "source": [
        "# setting up taining \n",
        "\n",
        "batch_size \u003d 64\n",
        "num_epochs \u003d 10\n",
        "\n",
        "#Adam\n",
        "base_lr \u003d 0.002\n",
        "adam \u003d Adam(lr\u003dbase_lr, beta_1\u003d0.9, beta_2\u003d0.999, epsilon\u003d1e-6, decay \u003d 0.0)\n",
        "ssd_loss \u003d SSDLoss(neg_pos_ratio\u003d2, n_neg_min\u003d0, alpha\u003d1.0, beta \u003d 1.0)\n",
        "model.compile(optimizer\u003dadam, loss\u003dssd_loss.compute_loss)\n",
        "\n",
        "\n",
        "ssd_box_encoder \u003d SSDBoxEncoder(img_height\u003dimg_height,\n",
        "                                img_width\u003dimg_width,\n",
        "                                n_classes\u003dn_classes, \n",
        "                                predictor_sizes\u003dpredictor_sizes,\n",
        "                                min_scale\u003dNone,\n",
        "                                max_scale\u003dNone,\n",
        "                                scales\u003dscales,\n",
        "                                aspect_ratios_global\u003dNone,\n",
        "                                aspect_ratios_per_layer\u003daspect_ratios,\n",
        "                                two_boxes_for_ar1\u003dtwo_boxes_for_ar1,\n",
        "                                limit_boxes\u003dlimit_boxes,\n",
        "                                variances\u003dvariances,\n",
        "                                pos_iou_threshold\u003d0.5,\n",
        "                                neg_iou_threshold\u003d0.2,\n",
        "                                coords\u003dcoords,\n",
        "                                normalize_coords\u003dnormalize_coords)\n",
        "\n",
        "train_dataset \u003d BatchGenerator(images_path\u003dtrain_data, \n",
        "                               include_classes\u003d\u0027all\u0027, \n",
        "                               box_output_format \u003d [\u0027class_id\u0027, \u0027xmin\u0027, \u0027xmax\u0027, \u0027ymin\u0027, \u0027ymax\u0027])\n",
        "\n",
        "print (\"TRAINING DATA\")\n",
        "\n",
        "train_dataset.parse_xml(\n",
        "                  annotations_path\u003dtrain_data,\n",
        "                  image_set_path\u003ddata_path,\n",
        "                  image_set\u003d\u0027None\u0027,\n",
        "                  classes \u003d class_names, \n",
        "                  exclude_truncated\u003dFalse,\n",
        "                  exclude_difficult\u003dFalse,\n",
        "                  ret\u003dFalse, \n",
        "                  debug \u003d False)\n",
        "\n",
        "train_generator \u003d train_dataset.generate(\n",
        "                 batch_size\u003dbatch_size,\n",
        "                 train\u003dTrue,\n",
        "                 ssd_box_encoder\u003dssd_box_encoder,\n",
        "                 equalize\u003dTrue,\n",
        "                 brightness\u003d(0.5,2,0.5),\n",
        "                 flip\u003d0.5,\n",
        "                 translate\u003d((0, 20), (0, 30), 0.5),\n",
        "                 scale\u003d(0.75, 1.2, 0.5),\n",
        "                 crop\u003dFalse,\n",
        "                 #random_crop \u003d (img_height,img_width,1,3), \n",
        "                 random_crop\u003dFalse,\n",
        "                 resize\u003d(img_height, img_width),\n",
        "                 #resize\u003dFalse,\n",
        "                 gray\u003dFalse,\n",
        "                 limit_boxes\u003dTrue,\n",
        "                 include_thresh\u003d0.4,\n",
        "                 diagnostics\u003dFalse)\n",
        "\n",
        "n_train_samples \u003d train_dataset.get_n_samples()\n",
        "\n",
        "print (\"Total number of training samples \u003d {}\".format(n_train_samples))\n",
        "\n",
        "\n",
        "print (\"VALIDATION DATA\")\n",
        "\n",
        "val_dataset \u003d BatchGenerator(images_path\u003dtest_data, include_classes\u003d\u0027all\u0027, \n",
        "                box_output_format \u003d [\u0027class_id\u0027, \u0027xmin\u0027, \u0027xmax\u0027, \u0027ymin\u0027, \u0027ymax\u0027])\n",
        "\n",
        "\n",
        "val_dataset.parse_xml(\n",
        "                  annotations_path\u003dtest_data,\n",
        "                  image_set_path\u003ddata_path,\n",
        "                  image_set\u003d\u0027None\u0027,\n",
        "                  classes \u003d class_names, \n",
        "                  exclude_truncated\u003dFalse,\n",
        "                  exclude_difficult\u003dFalse,\n",
        "                  ret\u003dFalse, \n",
        "                  debug \u003d False)\n",
        "\n",
        "\n",
        "val_generator \u003d val_dataset.generate(\n",
        "                 batch_size\u003dbatch_size,\n",
        "                 train\u003dTrue,\n",
        "                 ssd_box_encoder\u003dssd_box_encoder,\n",
        "                 equalize\u003dFalse,\n",
        "                 brightness\u003dFalse,\n",
        "                 flip\u003dFalse,\n",
        "                 translate\u003dFalse,\n",
        "                 scale\u003dFalse,\n",
        "                 crop\u003dFalse,\n",
        "                 #random_crop \u003d (img_height,img_width,1,3), \n",
        "                 random_crop\u003dFalse, \n",
        "                 resize\u003d(img_height, img_width), \n",
        "                 #resize\u003dFalse, \n",
        "                 gray\u003dFalse,\n",
        "                 limit_boxes\u003dTrue,\n",
        "                 include_thresh\u003d0.4,\n",
        "                 diagnostics\u003dFalse)\n",
        "\n",
        "n_val_samples \u003d val_dataset.get_n_samples()\n",
        "\n",
        "print (\"Total number of validation samples \u003d {}\".format(n_val_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-03-09T14:40:49.615390Z",
          "start_time": "2018-03-09T22:36:54.863941+08:00"
        },
        "pycharm": {},
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "lr remains 0.0020000000949949026\n",
            "4/4 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 41s 10s/step - loss: 0.1816 - val_loss: 0.1961\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.19611, saving model to ./models/ssd_mobilenet_face_epoch_01_loss0.1961.h5\n",
            "Epoch 2/10\n",
            "lr remains 0.0020000000949949026\n",
            "4/4 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 3s 787ms/step - loss: 0.1526 - val_loss: 0.1803\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.19611 to 0.18030, saving model to ./models/ssd_mobilenet_face_epoch_02_loss0.1803.h5\n",
            "Epoch 3/10\n",
            "lr remains 0.0020000000949949026\n",
            "4/4 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 3s 785ms/step - loss: 0.1389 - val_loss: 0.1685\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.18030 to 0.16846, saving model to ./models/ssd_mobilenet_face_epoch_03_loss0.1685.h5\n",
            "Epoch 4/10\n",
            "lr remains 0.0020000000949949026\n",
            "4/4 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 3s 821ms/step - loss: 0.1317 - val_loss: 0.1574\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.16846 to 0.15739, saving model to ./models/ssd_mobilenet_face_epoch_04_loss0.1574.h5\n",
            "Epoch 5/10\n",
            "lr remains 0.0020000000949949026\n",
            "4/4 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 13s 3s/step - loss: 0.1262 - val_loss: 0.1483\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.15739 to 0.14829, saving model to ./models/ssd_mobilenet_face_epoch_05_loss0.1483.h5\n",
            "Epoch 6/10\n",
            "lr remains 0.0020000000949949026\n",
            "4/4 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 15s 4s/step - loss: 0.1225 - val_loss: 0.1423\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.14829 to 0.14233, saving model to ./models/ssd_mobilenet_face_epoch_06_loss0.1423.h5\n",
            "Epoch 7/10\n",
            "lr remains 0.0020000000949949026\n",
            "4/4 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 16s 4s/step - loss: 0.1211 - val_loss: 0.1378\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.14233 to 0.13777, saving model to ./models/ssd_mobilenet_face_epoch_07_loss0.1378.h5\n",
            "Epoch 8/10\n",
            "lr remains 0.0020000000949949026\n",
            "4/4 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 15s 4s/step - loss: 0.1186 - val_loss: 0.1342\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.13777 to 0.13423, saving model to ./models/ssd_mobilenet_face_epoch_08_loss0.1342.h5\n",
            "Epoch 9/10\n",
            "lr remains 0.0020000000949949026\n",
            "4/4 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 15s 4s/step - loss: 0.1163 - val_loss: 0.1317\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.13423 to 0.13167, saving model to ./models/ssd_mobilenet_face_epoch_09_loss0.1317.h5\n",
            "Epoch 10/10\n",
            "lr remains 0.0020000000949949026\n",
            "4/4 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 14s 4s/step - loss: 0.1150 - val_loss: 0.1293\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.13167 to 0.12926, saving model to ./models/ssd_mobilenet_face_epoch_10_loss0.1293.h5\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name \u0027epochs\u0027 is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-9-47f39e702615\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                               validation_steps \u003d ceil(n_val_samples/batch_size))\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 35\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet_model_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\u0027ssd_mobilenet_weights_epoch_{}.h5\u0027\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"model and weight files saved at : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdet_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name \u0027epochs\u0027 is not defined"
          ]
        }
      ],
      "source": [
        "# now start the training \n",
        "\n",
        "def scheduler(epoch):\n",
        "  if epoch%10\u003d\u003d0 and epoch!\u003d0:\n",
        "    lr \u003d K.get_value(model.optimizer.lr)\n",
        "    K.set_value(model.optimizer.lr, lr*.95)\n",
        "    print(\"lr changed to {}\".format(lr*.95))\n",
        "  else: \n",
        "    print(\"lr remains {}\".format(K.get_value(model.optimizer.lr)))\n",
        "\n",
        "  return K.get_value(model.optimizer.lr)\n",
        "\n",
        "lr_schedule \u003d LearningRateScheduler(scheduler)\n",
        "\n",
        "plateau \u003d ReduceLROnPlateau(monitor\u003d\u0027val_loss\u0027, factor \u003d 0.3, patience \u003d4, epsilon\u003d0.001, cooldown\u003d0)\n",
        "tensorboard \u003d TensorBoard(log_dir\u003d\u0027./logs/trial1/\u0027, histogram_freq\u003d1, batch_size\u003d16, write_graph\u003dTrue, write_grads\u003dTrue, \n",
        "                          write_images\u003dTrue, embeddings_freq\u003d0, embeddings_layer_names\u003dNone, embeddings_metadata\u003dNone)\n",
        "early_stopping \u003d EarlyStopping(monitor\u003d\u0027val_loss\u0027, min_delta\u003d0.001, patience\u003d100)\n",
        "model_checkpoint \u003d  ModelCheckpoint(det_model_path + \u0027ssd_mobilenet_face_epoch_{epoch:02d}_loss{val_loss:.4f}.h5\u0027,\n",
        "                                                           monitor\u003d\u0027val_loss\u0027,\n",
        "                                                           verbose\u003d1,\n",
        "                                                           save_best_only\u003dTrue,\n",
        "                                                           save_weights_only\u003dTrue,\n",
        "                                                           mode\u003d\u0027auto\u0027,\n",
        "                                                           period\u003d1)\n",
        "\n",
        "\n",
        "history \u003d model.fit_generator(generator \u003d train_generator,\n",
        "                              steps_per_epoch \u003d ceil(n_train_samples/batch_size)*2,\n",
        "                              epochs \u003d num_epochs,\n",
        "                              callbacks \u003d [model_checkpoint, lr_schedule, early_stopping],                      \n",
        "                              validation_data \u003d val_generator,\n",
        "                              validation_steps \u003d ceil(n_val_samples/batch_size))\n",
        "\n",
        "# model.save_weights(det_model_path + \u0027ssd_mobilenet_weights_epoch_{}.h5\u0027.format(epochs))\n",
        "\n",
        "# print (\"model and weight files saved at : \" + det_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-03-09T15:11:52.562338Z",
          "start_time": "2018-03-09T23:11:52.383835+08:00"
        },
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mweights ./models/ssd_mobilenet_face_epoch_25_loss0.0916.h5 loaded\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "model_path \u003d \u0027./models/\u0027\n",
        "model_name \u003d \u0027ssd_mobilenet_face_epoch_25_loss0.0916.h5\u0027\n",
        "\n",
        "model.load_weights(model_path + model_name,  by_name\u003d True)\n",
        "\n",
        "print (colored(\u0027weights %s loaded\u0027 % (model_path + model_name), \u0027green\u0027))\n",
        "\n",
        "def save_bb(path, filename, results, prediction\u003dTrue):\n",
        "  \n",
        "  # print filename\n",
        "\n",
        "  img \u003d image.load_img(filename, target_size\u003d(img_height, img_width))\n",
        "  img \u003d image.img_to_array(img)\n",
        "\n",
        "  filename \u003d filename.split(\"/\")[-1]\n",
        "\n",
        "  if(not prediction):\n",
        "    filename \u003d filename[:-4] + \"_gt\" + \".jpg\"\n",
        "\n",
        "  #fig,currentAxis \u003d plt.subplots(1)\n",
        "  currentAxis \u003d plt.gca()\n",
        "\n",
        " # Get detections with confidence higher than 0.6.\n",
        "  colors \u003d plt.cm.hsv(np.linspace(0, 1, 25)).tolist()\n",
        "  color_code \u003d min(len(results), 16)\n",
        "  print (colored(\"total number of bbs: %d\" % len(results), \"yellow\"))\n",
        "  for result in results:\n",
        "    # Parse the outputs.\n",
        "\n",
        "    if(prediction):\n",
        "      det_label \u003d result[0]\n",
        "      det_conf \u003d result[1]\n",
        "      det_xmin \u003d result[2]\n",
        "      det_xmax \u003d result[3]\n",
        "      det_ymin \u003d result[4]\n",
        "      det_ymax \u003d result[5]\n",
        "    else :\n",
        "      det_label \u003d result[0]\n",
        "      det_xmin \u003d result[1]\n",
        "      det_xmax \u003d result[2]\n",
        "      det_ymin \u003d result[3]\n",
        "      det_ymax \u003d result[4]\n",
        "\n",
        "    xmin \u003d int(det_xmin)\n",
        "    ymin \u003d int(det_ymin)\n",
        "    xmax \u003d int(det_xmax)\n",
        "    ymax \u003d int(det_ymax)\n",
        "\n",
        "    if(prediction):\n",
        "      score \u003d det_conf\n",
        "    \n",
        "    plt.imshow(img / 255.)\n",
        "    \n",
        "    label \u003d int(int(det_label))\n",
        "    label_name \u003d class_names[label]\n",
        "    # print label_name \n",
        "    # print label\n",
        "\n",
        "    if(prediction):\n",
        "      display_txt \u003d \u0027{:0.2f}\u0027.format(score)\n",
        "    else:\n",
        "      display_txt \u003d \u0027{}\u0027.format(label_name)\n",
        "\n",
        "      \n",
        "    # print (xmin, ymin, ymin, ymax)\n",
        "    coords \u003d (xmin, ymin), (xmax-xmin), (ymax-ymin)\n",
        "    color_code \u003d color_code-1 \n",
        "    color \u003d colors[color_code]\n",
        "    currentAxis.add_patch(plt.Rectangle(*coords, fill\u003dFalse, edgecolor\u003dcolor, linewidth\u003d2))\n",
        "    currentAxis.text(xmin, ymin, display_txt, bbox\u003d{\u0027facecolor\u0027:color, \u0027alpha\u0027:0.2})\n",
        "\n",
        "  # y 轴不可见\n",
        "  currentAxis.axes.get_yaxis().set_visible(False)\n",
        "  # x 轴不可见\n",
        "  currentAxis.axes.get_xaxis().set_visible(False)\n",
        "  plt.savefig(path + filename, bbox_inches\u003d\u0027tight\u0027)\n",
        "\n",
        "  print (\u0027saved\u0027 , path + filename)\n",
        "\n",
        "  plt.clf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-03-09T15:42:25.277298Z",
          "start_time": "2018-03-09T23:42:06.631484+08:00"
        },
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mdone.\u001b[0m\n",
            "\u001b[33mnow predicting...\u001b[0m\n",
            "\u001b[33mtotal number of bbs: 1\u001b[0m\n",
            "saved ./output_test/18_Concerts_Concerts_18_102.jpg\n",
            "\u001b[33mtotal number of bbs: 1\u001b[0m\n",
            "saved ./output_test/18_Concerts_Concerts_18_102_gt.jpg\n",
            "\u001b[33mtotal number of bbs: 0\u001b[0m\n",
            "saved ./output_test/25_Soldier_Patrol_Soldier_Patrol_25_467.jpg\n",
            "\u001b[33mtotal number of bbs: 1\u001b[0m\n",
            "saved ./output_test/25_Soldier_Patrol_Soldier_Patrol_25_467_gt.jpg\n",
            "\u001b[33mtotal number of bbs: 1\u001b[0m\n",
            "saved ./output_test/47_Matador_Bullfighter_matadorbullfighting_47_236.jpg\n",
            "\u001b[33mtotal number of bbs: 27\u001b[0m\n",
            "saved ./output_test/47_Matador_Bullfighter_matadorbullfighting_47_236_gt.jpg\n",
            "\u001b[33mtotal number of bbs: 0\u001b[0m\n",
            "saved ./output_test/41_Swimming_Swimmer_41_68.jpg\n",
            "\u001b[33mtotal number of bbs: 1\u001b[0m\n",
            "saved ./output_test/41_Swimming_Swimmer_41_68_gt.jpg\n",
            "\u001b[33mtotal number of bbs: 6\u001b[0m\n",
            "saved ./output_test/35_Basketball_basketballgame_ball_35_998.jpg\n",
            "\u001b[33mtotal number of bbs: 26\u001b[0m\n",
            "saved ./output_test/35_Basketball_basketballgame_ball_35_998_gt.jpg\n",
            "\u001b[33mtotal number of bbs: 0\u001b[0m\n",
            "saved ./output_test/10_People_Marching_People_Marching_10_People_Marching_People_Marching_10_People_Marching_People_Marching_10_674.jpg\n",
            "\u001b[33mtotal number of bbs: 1\u001b[0m\n",
            "saved ./output_test/10_People_Marching_People_Marching_10_People_Marching_People_Marching_10_People_Marching_People_Marching_10_674_gt.jpg\n",
            "\u001b[33mtotal number of bbs: 3\u001b[0m\n",
            "saved ./output_test/28_Sports_Fan_Sports_Fan_28_590.jpg\n",
            "\u001b[33mtotal number of bbs: 9\u001b[0m\n",
            "saved ./output_test/28_Sports_Fan_Sports_Fan_28_590_gt.jpg\n",
            "\u001b[33mtotal number of bbs: 7\u001b[0m\n",
            "saved ./output_test/20_Family_Group_Family_Group_20_696.jpg\n",
            "\u001b[33mtotal number of bbs: 7\u001b[0m\n",
            "saved ./output_test/20_Family_Group_Family_Group_20_696_gt.jpg\n",
            "\u001b[33mtotal number of bbs: 4\u001b[0m\n",
            "saved ./output_test/35_Basketball_playingbasketball_35_417.jpg\n",
            "\u001b[33mtotal number of bbs: 16\u001b[0m\n",
            "saved ./output_test/35_Basketball_playingbasketball_35_417_gt.jpg\n",
            "\u001b[33mtotal number of bbs: 0\u001b[0m\n",
            "saved ./output_test/41_Swimming_Swimmer_41_68.jpg\n",
            "\u001b[33mtotal number of bbs: 1\u001b[0m\n",
            "saved ./output_test/41_Swimming_Swimmer_41_68_gt.jpg\n",
            "\u001b[33mtotal number of bbs: 0\u001b[0m\n",
            "saved ./output_test/12_Group_Team_Organized_Group_12_Group_Team_Organized_Group_12_101.jpg\n",
            "\u001b[33mtotal number of bbs: 10\u001b[0m\n",
            "saved ./output_test/12_Group_Team_Organized_Group_12_Group_Team_Organized_Group_12_101_gt.jpg\n",
            "\u001b[33mtotal number of bbs: 2\u001b[0m\n",
            "saved ./output_test/45_Balloonist_Balloonist_45_225.jpg\n",
            "\u001b[33mtotal number of bbs: 3\u001b[0m\n",
            "saved ./output_test/45_Balloonist_Balloonist_45_225_gt.jpg\n",
            "\u001b[33mtotal number of bbs: 1\u001b[0m\n",
            "saved ./output_test/49_Greeting_peoplegreeting_49_589.jpg\n",
            "\u001b[33mtotal number of bbs: 1\u001b[0m\n",
            "saved ./output_test/49_Greeting_peoplegreeting_49_589_gt.jpg\n",
            "\u001b[33mtotal number of bbs: 1\u001b[0m\n",
            "saved ./output_test/57_Angler_peoplefishing_57_153.jpg\n",
            "\u001b[33mtotal number of bbs: 1\u001b[0m\n",
            "saved ./output_test/57_Angler_peoplefishing_57_153_gt.jpg\n",
            "\u001b[33mtotal number of bbs: 2\u001b[0m\n",
            "saved ./output_test/22_Picnic_Picnic_22_561.jpg\n",
            "\u001b[33mtotal number of bbs: 2\u001b[0m\n",
            "saved ./output_test/22_Picnic_Picnic_22_561_gt.jpg\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-14-7f21759862bf\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuppress\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 49\u001b[0;31m   \u001b[0msave_bb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./output_test/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_decoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m   \u001b[0msave_bb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./output_test/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing import image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "test_size \u003d 16\n",
        "test_generator \u003d val_dataset.generate(\n",
        "                 batch_size\u003dtest_size,\n",
        "                 train\u003dFalse,\n",
        "                 ssd_box_encoder\u003dssd_box_encoder,\n",
        "                 equalize\u003dFalse,\n",
        "                 brightness\u003dFalse,\n",
        "                 flip\u003dFalse,\n",
        "                 translate\u003dFalse,\n",
        "                 scale\u003dFalse,\n",
        "                 crop\u003dFalse,\n",
        "                 #random_crop \u003d (img_height,img_width,1,3), \n",
        "                 random_crop\u003dFalse, \n",
        "                 resize\u003d(img_height, img_width), \n",
        "                 #resize\u003dFalse,\n",
        "                 gray\u003dFalse,\n",
        "                 limit_boxes\u003dTrue,\n",
        "                 include_thresh\u003d0.4,\n",
        "                 diagnostics\u003dFalse)\n",
        "\n",
        "print (colored(\"done.\", \"green\"))\n",
        "\n",
        "print (colored(\"now predicting...\", \"yellow\"))\n",
        "\n",
        "_CONF \u003d 0.60 \n",
        "_IOU \u003d 0.15\n",
        "\n",
        "for i in range(test_size):\n",
        "  X, y, filenames \u003d next(test_generator)\n",
        "\n",
        "  y_pred \u003d model.predict(X)\n",
        "\n",
        "\n",
        "  y_pred_decoded \u003d decode_y2(y_pred,\n",
        "                             confidence_thresh\u003d_CONF,\n",
        "                            iou_threshold\u003d_IOU,\n",
        "                            top_k\u003d\u0027all\u0027,\n",
        "                            input_coords\u003dcoords,\n",
        "                            normalize_coords\u003dnormalize_coords,\n",
        "                            img_height\u003dimg_height,\n",
        "                            img_width\u003dimg_width)\n",
        "\n",
        "\n",
        "  np.set_printoptions(suppress\u003dTrue)\n",
        "\n",
        "  save_bb(\"./output_test/\", filenames[i], y_pred_decoded[i])\n",
        "  save_bb(\"./output_test/\", filenames[i], y[i], prediction\u003dFalse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}